{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T06:07:25.051457800Z",
     "start_time": "2026-01-08T06:07:25.033773900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n"
   ],
   "id": "f4acbc4c249bfdc9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T06:07:25.643619300Z",
     "start_time": "2026-01-08T06:07:25.619812900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "im_shape = (250, 250)\n",
    "seed = 10\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_LIMIT = 10000  # Limit training samples\n",
    "TEST_LIMIT = None    # Use all test samples\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ],
   "id": "1991dd184ca51e30",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T06:07:28.337985Z",
     "start_time": "2026-01-08T06:07:26.292638100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset from Hugging Face Hub\n",
    "print(\"Loading dataset from Hugging Face Hub...\")\n",
    "dataset = load_dataset(\"ysif9/fruit-recognition\")\n",
    "\n",
    "print(f\"Original dataset sizes:\")\n",
    "print(f\"  Train: {len(dataset['train'])} samples\")\n",
    "print(f\"  Test: {len(dataset['test'])} samples\")\n"
   ],
   "id": "b4fe0c1ea695400c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face Hub...\n",
      "Original dataset sizes:\n",
      "  Train: 25659 samples\n",
      "  Test: 7070 samples\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T06:08:04.274419900Z",
     "start_time": "2026-01-08T06:08:04.244655900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing function to handle images\n",
    "def preprocess_image(example, im_shape=(250, 250)):\n",
    "    \"\"\"\n",
    "    Preprocess images: convert to RGB, resize, and normalize.\n",
    "    Handles RGBA, grayscale, and palette images.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    img = example['image']\n",
    "    \n",
    "    # Convert PIL Image to RGB if needed (handles RGBA, P, L modes)\n",
    "    if isinstance(img, Image.Image):\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "    \n",
    "    # Resize image\n",
    "    img = img.resize(im_shape)\n",
    "    \n",
    "    # Convert to numpy array and normalize to [0, 1]\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    example['image'] = img_array\n",
    "    return example\n"
   ],
   "id": "3c679204e51a624c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-08T06:08:04.711293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply preprocessing and set format for TensorFlow\n",
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "# Apply limit to training data if specified\n",
    "train_dataset = dataset['train']\n",
    "if TRAIN_LIMIT is not None and TRAIN_LIMIT < len(train_dataset):\n",
    "    train_dataset = train_dataset.select(range(TRAIN_LIMIT))\n",
    "    print(f\"Using {TRAIN_LIMIT} training samples\")\n",
    "\n",
    "# Apply limit to test data if specified\n",
    "test_dataset = dataset['test']\n",
    "if TEST_LIMIT is not None and TEST_LIMIT < len(test_dataset):\n",
    "    test_dataset = test_dataset.select(range(TEST_LIMIT))\n",
    "    print(f\"Using {TEST_LIMIT} test samples\")\n",
    "\n",
    "# Preprocess images\n",
    "train_dataset = train_dataset.map(preprocess_image, num_proc=4)\n",
    "test_dataset = test_dataset.map(preprocess_image, num_proc=4)\n"
   ],
   "id": "e5c724d5f797ff40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n",
      "Using 10000 training samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  40%|███▉      | 3985/10000 [00:24<01:30, 66.45 examples/s]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get number of classes\n",
    "num_classes = len(set(train_dataset['label']))\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ],
   "id": "fcad174bab57d187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert to TensorFlow datasets using the built-in method\n",
    "print(\"Converting to TensorFlow datasets...\")\n",
    "\n",
    "# Convert to tf.data.Dataset\n",
    "train_tf_dataset = train_dataset.to_tf_dataset(\n",
    "    columns=['image'],\n",
    "    label_cols=['label'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=None\n",
    ")\n",
    "\n",
    "test_tf_dataset = test_dataset.to_tf_dataset(\n",
    "    columns=['image'],\n",
    "    label_cols=['label'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=None\n",
    ")\n"
   ],
   "id": "7a0e2af6d8109519"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create validation split from training data (20%)\n",
    "train_size = len(train_dataset)\n",
    "val_size = int(0.2 * train_size)\n",
    "train_size_adjusted = train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_tf_dataset_split = train_tf_dataset.take(train_size_adjusted // BATCH_SIZE)\n",
    "validation_tf_dataset = train_tf_dataset.skip(train_size_adjusted // BATCH_SIZE)\n",
    "\n",
    "print(f\"Training batches: ~{train_size_adjusted // BATCH_SIZE}\")\n",
    "print(f\"Validation batches: ~{val_size // BATCH_SIZE}\")\n",
    "print(f\"Test batches: ~{len(test_dataset) // BATCH_SIZE}\")\n"
   ],
   "id": "41060c18c3d9975d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encode labels\n",
    "def one_hot_encode(image, label):\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "train_tf_dataset_split = train_tf_dataset_split.map(one_hot_encode)\n",
    "validation_tf_dataset = validation_tf_dataset.map(one_hot_encode)\n",
    "test_tf_dataset = test_tf_dataset.map(one_hot_encode)\n",
    "\n",
    "# Prefetch for performance\n",
    "train_tf_dataset_split = train_tf_dataset_split.prefetch(tf.data.AUTOTUNE)\n",
    "validation_tf_dataset = validation_tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_tf_dataset = test_tf_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ],
   "id": "1a925d5a6d6356ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Conv2D(20, kernel_size=(3, 3), activation='relu', \n",
    "           input_shape=(im_shape[0], im_shape[1], 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(40, kernel_size=(3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ],
   "id": "420a0478a2d7eaff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training\n",
    "epochs = 1\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/model_improved.h5',\n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, \n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_tf_dataset_split,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=validation_tf_dataset,\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "ae36d32789836ceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the best saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model_improved.h5')\n"
   ],
   "id": "5e5150d7ee86c90e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_loss, val_accuracy = model.evaluate(validation_tf_dataset)\n",
    "print(f'Validation loss: {val_loss:.4f}')\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')\n"
   ],
   "id": "99827aff8a18d41a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_tf_dataset)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n"
   ],
   "id": "371ceda95464ec0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate predictions for confusion matrix and classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_tf_dataset:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n"
   ],
   "id": "6986762db54eb907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get class names (assuming they're numeric labels 0, 1, 2, ...)\n",
    "classes = [str(i) for i in range(num_classes)]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "fb5be1ad8dea33b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Classification Report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ],
   "id": "5a42ebffcc545621"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "35d1fc9dac44efdd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
